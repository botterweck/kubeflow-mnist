{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea5a454-49f8-4a4c-878e-5915c8624aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Base Image\n",
    "\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "from kfp.dsl import Input, Output\n",
    "from kfp.dsl import Dataset, Artifact\n",
    "from kfp.dsl import Model, Metrics, ClassificationMetrics\n",
    "\n",
    "from typing import NamedTuple\n",
    "\n",
    "BASE_IMAGE = 'nvcr.io/nvidia/tensorflow:25.01-tf2-py3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65d174d-5fb2-4784-911b-016ee3962406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "def load_data(\n",
    "    x_train_pickle: Output[Dataset],\n",
    "    y_train_pickle: Output[Dataset],\n",
    "    x_test_pickle: Output[Dataset],\n",
    "    y_test_pickle: Output[Dataset],\n",
    "):\n",
    "    # import dataset\n",
    "    from keras.datasets import mnist\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "\n",
    "    # load dataset\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # count the number of unique train labels\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    print(\"Train labels: \", dict(zip(unique, counts)))\n",
    "\n",
    "    # count the number of unique test labels\n",
    "    unique, counts = np.unique(y_test, return_counts=True)\n",
    "    print(\"\\nTest labels: \", dict(zip(unique, counts)))\n",
    "\n",
    "    with open(x_train_pickle.path, \"wb\") as file:\n",
    "        pickle.dump(x_train, file)\n",
    "\n",
    "    with open(y_train_pickle.path, \"wb\") as file:\n",
    "        pickle.dump(y_train, file)\n",
    "\n",
    "    with open(x_test_pickle.path, \"wb\") as file:\n",
    "        pickle.dump(x_test, file)\n",
    "\n",
    "    with open(y_test_pickle.path, \"wb\") as file:\n",
    "        pickle.dump(y_test, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f7a2631-3086-417d-8381-915ee7eae51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data \n",
    "\n",
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def preprocess_data(\n",
    "    x_train_pickle: Input[Dataset],\n",
    "    y_train_pickle: Input[Dataset],\n",
    "    x_test_pickle: Input[Dataset],\n",
    "    y_test_pickle: Input[Dataset],\n",
    "    x_train_prep: Output[Dataset], # Output preprocessed training data (needed for quantization)\n",
    "    y_train_prep: Output[Dataset],\n",
    "    x_test_prep: Output[Dataset],\n",
    "    y_test_prep: Output[Dataset],\n",
    ") -> NamedTuple(\"outputs\", input_size=int, num_labels=int):\n",
    "\n",
    "    from keras.utils import to_categorical\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from typing import NamedTuple\n",
    "\n",
    "    with open(x_train_pickle.path, \"rb\") as file:\n",
    "        x_train = pickle.load(file)\n",
    "\n",
    "    with open(y_train_pickle.path, \"rb\") as file:\n",
    "        y_train = pickle.load(file)\n",
    "\n",
    "    with open(x_test_pickle.path, \"rb\") as file:\n",
    "        x_test = pickle.load(file)\n",
    "\n",
    "    with open(y_test_pickle.path, \"rb\") as file:\n",
    "        y_test = pickle.load(file)\n",
    "\n",
    "    num_labels = len(np.unique(y_train))\n",
    "\n",
    "    # Reshape and normalize training data (needed for quantization calibration)\n",
    "    image_size = x_train.shape[1]\n",
    "    input_size = image_size * image_size\n",
    "    x_train_processed = np.reshape(x_train, [-1, input_size])\n",
    "    x_train_processed = x_train_processed.astype(\"float32\") / 255\n",
    "\n",
    "    # Reshape and normalize test data\n",
    "    x_test_processed = np.reshape(x_test, [-1, input_size])\n",
    "    x_test_processed = x_test_processed.astype(\"float32\") / 255\n",
    "\n",
    "    # One-hot encode labels\n",
    "    y_train_processed = to_categorical(y_train)\n",
    "    y_test_processed = to_categorical(y_test)\n",
    "\n",
    "    # Save processed data\n",
    "    with open(x_train_prep.path, \"wb\") as file:\n",
    "        pickle.dump(x_train_processed, file)\n",
    "\n",
    "    with open(y_train_prep.path, \"wb\") as file:\n",
    "        pickle.dump(y_train_processed, file)\n",
    "\n",
    "    with open(x_test_prep.path, \"wb\") as file:\n",
    "        pickle.dump(x_test_processed, file)\n",
    "\n",
    "    with open(y_test_prep.path, \"wb\") as file:\n",
    "        pickle.dump(y_test_processed, file)\n",
    "\n",
    "    outputs = NamedTuple(\"outputs\", input_size=int, num_labels=int)\n",
    "    return outputs(input_size, num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d6fe72f-17cd-4cff-aab1-e8b37e12c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE\n",
    ")\n",
    "def train(\n",
    "    input_size: int,\n",
    "    num_labels: int,\n",
    "    epochs: int,\n",
    "    x_train_pickle: Input[Dataset],\n",
    "    y_train_pickle: Input[Dataset],\n",
    "    model_artifact: Output[Model],\n",
    "    log: Output[Artifact],\n",
    "):\n",
    "    from keras.callbacks import TensorBoard\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, Dropout\n",
    "    import pickle\n",
    "    import os\n",
    "    from datetime import datetime\n",
    "\n",
    "    import tensorflow as tf\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if not gpus:\n",
    "        print(\"TensorFlow CANNOT see any GPUs. GPU acceleration is NOT possible.\")\n",
    "    else:\n",
    "        print(f\"TensorFlow found {len(gpus)} GPU(s): {gpus}\")\n",
    "\n",
    "    batch_size = 128\n",
    "    hidden_units = 256\n",
    "    dropout = 0.45\n",
    "\n",
    "    with open(x_train_pickle.path, \"rb\") as file:\n",
    "        x_train = pickle.load(file)\n",
    "\n",
    "    with open(y_train_pickle.path, \"rb\") as file:\n",
    "        y_train = pickle.load(file)\n",
    "\n",
    "\n",
    "    log_dir = f\"{log.path}/logs/fit/{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_units, input_dim=input_size))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(hidden_units))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(num_labels))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[tensorboard_callback],\n",
    "    )\n",
    "\n",
    "    # model.save(model_artifact.path)  \n",
    "    \n",
    "    os.makedirs(model_artifact.path, exist_ok=True)\n",
    "    model_path = os.path.join(model_artifact.path, \"model.keras\")\n",
    "    model.save(model_path)\n",
    "    print(f\"[train] model saved to {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e0d1dcf-1fe9-4fc3-b09f-e31ab752e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"scikit-learn\"],\n",
    ")\n",
    "def evaluate(\n",
    "    model_artifact: Input[Model],\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    scalar_metrics: Output[Metrics],\n",
    "    x_test_pickle: Input[Dataset],\n",
    "    y_test_pickle: Input[Dataset],\n",
    "):\n",
    "    from keras.models import load_model\n",
    "    from keras.metrics import Precision\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    import os\n",
    "    model_path = os.path.join(model_artifact.path, \"model.keras\")\n",
    "    print(f\"[evaluate] loading model from {model_path}\")\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    with open(x_test_pickle.path, \"rb\") as file:\n",
    "        x_test = pickle.load(file)\n",
    "\n",
    "    with open(y_test_pickle.path, \"rb\") as file:\n",
    "        y_test = pickle.load(file)\n",
    "\n",
    "    predictions = model.predict(x_test, batch_size=batch_size)\n",
    "    predictions = (predictions >= 0.5).astype(int)\n",
    "\n",
    "    metrics.log_confusion_matrix(\n",
    "        [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n",
    "        confusion_matrix(\n",
    "            y_test.argmax(axis=1), predictions.argmax(axis=1)\n",
    "        ).tolist(),  # .tolist() to convert np array to list.\n",
    "    )\n",
    "    m = Precision()\n",
    "    m.update_state(y_test, predictions)\n",
    "    loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    scalar_metrics.log_metric(\"accuracy\", acc)\n",
    "    scalar_metrics.log_metric(\"loss\", loss)\n",
    "    scalar_metrics.log_metric(\"precision\", m.result().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eadeeca5-693e-47b8-97d7-9aecd2862f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate fp32 model\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"scikit-learn\"],\n",
    ")\n",
    "def evaluate_fp32_model(\n",
    "    model_artifact: Input[Model], # Input is the directory containing the Keras model\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    scalar_metrics: Output[Metrics],\n",
    "    x_test_prep: Input[Dataset], # Changed input name for clarity\n",
    "    y_test_prep: Input[Dataset], # Changed input name for clarity\n",
    "):\n",
    "    from keras.models import load_model\n",
    "    from keras.metrics import Precision # Note: Keras Precision might not be ideal here\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    # Load the Keras model\n",
    "    model_dir_path = os.path.join(model_artifact.path, \"fp32_model\")\n",
    "    model_path = os.path.join(model_dir_path, \"model.keras\")\n",
    "    print(f\"[evaluate_fp32] loading model from {model_path}\")\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    batch_size = 128\n",
    "\n",
    "    with open(x_test_prep.path, \"rb\") as file: # Changed path name\n",
    "        x_test = pickle.load(file)\n",
    "\n",
    "    with open(y_test_prep.path, \"rb\") as file: # Changed path name\n",
    "        y_test = pickle.load(file) # y_test is one-hot encoded\n",
    "\n",
    "    # Evaluate using Keras model.evaluate\n",
    "    loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    print(f\"[evaluate_fp32] Keras Evaluation - Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Get predictions for confusion matrix and other metrics\n",
    "    y_pred_proba = model.predict(x_test, batch_size=batch_size)\n",
    "    y_pred_labels = np.argmax(y_pred_proba, axis=1)\n",
    "    y_true_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Calculate metrics using sklearn\n",
    "    # Use macro average for precision as it's multi-class\n",
    "    precision = precision_score(y_true_labels, y_pred_labels, average='macro', zero_division=0)\n",
    "    print(f\"[evaluate_fp32] Sklearn Metrics - Accuracy: {acc:.4f}, Precision (macro): {precision:.4f}\")\n",
    "\n",
    "    # Log metrics\n",
    "    scalar_metrics.log_metric(\"fp32_accuracy\", float(acc))\n",
    "    scalar_metrics.log_metric(\"fp32_loss\", float(loss))\n",
    "    scalar_metrics.log_metric(\"fp32_precision_macro\", float(precision))\n",
    "\n",
    "    metrics.log_confusion_matrix(\n",
    "        [str(i) for i in range(10)], # Class names '0' through '9'\n",
    "        confusion_matrix(y_true_labels, y_pred_labels).tolist(), # Convert np array to list\n",
    "    )\n",
    "    print(\"[evaluate_fp32] Metrics logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9076cf1-c7ee-4305-9c77-5029d709c261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate quantized model\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE,\n",
    "    packages_to_install=[\"scikit-learn\"],\n",
    ")\n",
    "def evaluate_quantized_model(\n",
    "    quantized_model_artifact: Input[Model], # Input is the directory containing .tflite\n",
    "    metrics: Output[ClassificationMetrics],\n",
    "    scalar_metrics: Output[Metrics],\n",
    "    x_test_prep: Input[Dataset],\n",
    "    y_test_prep: Input[Dataset],\n",
    "):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import os\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
    "\n",
    "    # Load the TFLite model and allocate tensors\n",
    "    tflite_model_path = os.path.join(quantized_model_artifact.path, 'quantized_model.tflite')\n",
    "    print(f\"[evaluate_quantized] Loading TFLite model from: {tflite_model_path}\")\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output tensor details\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "    input_dtype = input_details['dtype']\n",
    "    output_dtype = output_details['dtype']\n",
    "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "    output_scale, output_zero_point = output_details[\"quantization\"]\n",
    "    print(f\"[evaluate_quantized] Input Details: {input_details}\")\n",
    "    print(f\"[evaluate_quantized] Output Details: {output_details}\")\n",
    "\n",
    "    # Load test data\n",
    "    print(f\"[evaluate_quantized] Loading test data...\")\n",
    "    with open(x_test_prep.path, \"rb\") as file:\n",
    "        x_test_float = pickle.load(file) # Load original float32 data\n",
    "\n",
    "    with open(y_test_prep.path, \"rb\") as file:\n",
    "        y_test_one_hot = pickle.load(file)\n",
    "    y_true_labels = np.argmax(y_test_one_hot, axis=1)\n",
    "\n",
    "    print(f\"[evaluate_quantized] Quantizing input data to {input_dtype}...\")\n",
    "    # Quantize input data according to input tensor details\n",
    "    # Formula: int_value = float_value / scale + zero_point\n",
    "    x_test_quantized = (x_test_float / input_scale) + input_zero_point\n",
    "    x_test_quantized = x_test_quantized.astype(input_dtype)\n",
    "    print(f\"[evaluate_quantized] Input data shape: {x_test_quantized.shape}, dtype: {x_test_quantized.dtype}\")\n",
    "\n",
    "\n",
    "    # Run inference\n",
    "    print(f\"[evaluate_quantized] Running inference on {len(x_test_quantized)} samples...\")\n",
    "    y_pred_quantized_list = []\n",
    "    for i in range(len(x_test_quantized)):\n",
    "        interpreter.set_tensor(input_details['index'], np.expand_dims(x_test_quantized[i], axis=0))\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details['index'])\n",
    "        y_pred_quantized_list.append(output_data[0]) # Remove batch dim\n",
    "\n",
    "    y_pred_quantized = np.array(y_pred_quantized_list)\n",
    "    print(f\"[evaluate_quantized] Inference complete. Output shape: {y_pred_quantized.shape}, dtype: {y_pred_quantized.dtype}\")\n",
    "\n",
    "    # Dequantize output predictions to calculate loss/metrics easily\n",
    "    # Formula: float_value = (int_value - zero_point) * scale\n",
    "    y_pred_float = (y_pred_quantized.astype(np.float32) - output_zero_point) * output_scale\n",
    "    print(f\"[evaluate_quantized] Dequantized output shape: {y_pred_float.shape}, dtype: {y_pred_float.dtype}\")\n",
    "\n",
    "    # Get predicted labels\n",
    "    y_pred_labels = np.argmax(y_pred_float, axis=1)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true_labels, y_pred_labels)\n",
    "    # Use macro average for precision as it's multi-class\n",
    "    precision = precision_score(y_true_labels, y_pred_labels, average='macro', zero_division=0)\n",
    "    # Calculate categorical cross-entropy loss (requires probabilities)\n",
    "    # Softmax the dequantized outputs\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "    y_pred_proba = softmax(y_pred_float)\n",
    "    # Compute loss - use stable version\n",
    "    N = y_pred_proba.shape[0]\n",
    "    log_likelihood = -np.log(y_pred_proba[range(N), y_true_labels] + 1e-9) # Add epsilon for stability\n",
    "    loss = np.sum(log_likelihood) / N\n",
    "\n",
    "    print(f\"[evaluate_quantized] Metrics - Accuracy: {accuracy:.4f}, Precision (macro): {precision:.4f}, Loss: {loss:.4f}\")\n",
    "\n",
    "    # Log metrics\n",
    "    scalar_metrics.log_metric(\"quantized_int8_accuracy\", float(accuracy))\n",
    "    scalar_metrics.log_metric(\"quantized_int8_loss\", float(loss))\n",
    "    scalar_metrics.log_metric(\"quantized_int8_precision_macro\", float(precision))\n",
    "\n",
    "    metrics.log_confusion_matrix(\n",
    "        [str(i) for i in range(10)], # Class names '0' through '9'\n",
    "        confusion_matrix(y_true_labels, y_pred_labels).tolist(), # Convert np array to list\n",
    "    )\n",
    "    print(\"[evaluate_quantized] Metrics logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ece74042-5430-443a-ab9e-3b6a7299c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"mnist-pipeline-tf-gpu\",\n",
    ")\n",
    "def mnist_pipeline(epochs: int):\n",
    "    data = (\n",
    "        load_data()\n",
    "        .set_memory_limit(\"4G\")\n",
    "        .set_memory_request(\"4G\")\n",
    "        .set_cpu_limit(\"2\")\n",
    "        .set_cpu_request(\"2\")\n",
    "    )\n",
    "    preprocess = (\n",
    "        preprocess_data(\n",
    "            x_train_pickle=data.outputs[\"x_train_pickle\"],\n",
    "            y_train_pickle=data.outputs[\"y_train_pickle\"],\n",
    "            x_test_pickle=data.outputs[\"x_test_pickle\"],\n",
    "            y_test_pickle=data.outputs[\"y_test_pickle\"],\n",
    "        )\n",
    "        .set_memory_limit(\"4G\")\n",
    "        .set_memory_request(\"4G\")\n",
    "        .set_cpu_limit(\"1\")\n",
    "        .set_cpu_request(\"1\")\n",
    "    )\n",
    "    preprocess.after(data)\n",
    "    model = (\n",
    "        train(\n",
    "            input_size=preprocess.outputs[\"input_size\"],\n",
    "            num_labels=preprocess.outputs[\"num_labels\"],\n",
    "            epochs=epochs,\n",
    "            x_train_pickle=preprocess.outputs[\"x_train_prep\"],\n",
    "            y_train_pickle=preprocess.outputs[\"y_train_prep\"],\n",
    "        )\n",
    "        .set_gpu_limit(1)\n",
    "        .add_node_selector_constraint('nvidia.com/gpu')\n",
    "        .set_memory_limit(\"2G\")\n",
    "        .set_memory_request(\"1G\")\n",
    "        .set_cpu_limit(\"1\")\n",
    "        .set_cpu_request(\"0.5\")        \n",
    "    )\n",
    "    model.after(preprocess)\n",
    "    evaluation = (\n",
    "        evaluate(\n",
    "            model_artifact=model.outputs[\"model_artifact\"],\n",
    "            x_test_pickle=preprocess.outputs[\"x_test_prep\"],\n",
    "            y_test_pickle=preprocess.outputs[\"y_test_prep\"],\n",
    "        )\n",
    "        .set_memory_limit(\"4G\")\n",
    "        .set_memory_request(\"4G\")\n",
    "        .set_cpu_limit(\"1\")\n",
    "        .set_cpu_request(\"1\")\n",
    "    )\n",
    "    evaluation.after(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dd89793-ed7a-4a4d-95e8-f45ec73544ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/ceeeb68d-a201-4e62-b693-c2997177635e\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/9a5deb68-8570-4e33-b106-a15552bf7c4c\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline run submitted. Run details: 9a5deb68-8570-4e33-b106-a15552bf7c4c\n"
     ]
    }
   ],
   "source": [
    "# Run pipeline (\n",
    "client = kfp.Client()\n",
    "\n",
    "# Make sure KFP client points to your Kubeflow endpoint if needed\n",
    "# client = kfp.Client(host='<your-kubeflow-pipelines-url>')\n",
    "\n",
    "run = client.create_run_from_pipeline_func(\n",
    "    mnist_pipeline,\n",
    "    arguments={\"epochs\": 3},\n",
    "    experiment_name=\"mnist_pipeline\",\n",
    ")\n",
    "\n",
    "print(f\"Pipeline run submitted. Run details: {run.run_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc75e9-b258-40b4-a3b3-fbe57e3b70e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
