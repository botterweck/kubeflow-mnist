{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73c9fa01-5551-4569-8332-9cd638b9de53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch Base Image: nvcr.io/nvidia/pytorch:24.01-py3\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "\n",
    "# https://hub.docker.com/r/kubeflowkatib/pytorch-mnist-gpu/tags\n",
    "# https://github.com/kubeflow/katib/blob/master/examples/v1beta1/trial-images/pytorch-mnist/Dockerfile.gpu\n",
    "# nvcr.io/nvidia/pytorch:24.01-py3 <-- base image\n",
    "# https://docs.nvidia.com/deeplearning/frameworks/index.html\n",
    " \n",
    "# BASE_IMAGE = 'tensorflow/tensorflow:latest-gpu' # TensorFlow CANNOT see any GPUs. GPU acceleration is NOT possible.\n",
    "# BASE_IMAGE = 'kubeflowkatib/pytorch-mnist-gpu' # does not finish?\n",
    "# BASE_IMAGE = 'nvcr.io/nvidia/pytorch:25.01-py3'\n",
    "BASE_IMAGE = 'nvcr.io/nvidia/pytorch:24.01-py3'\n",
    "print(f\"Using PyTorch Base Image: {BASE_IMAGE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05bb9266-1deb-4179-9a57-642236d11e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=BASE_IMAGE,\n",
    ")\n",
    "def check_gpu_access_pytorch():\n",
    "    \"\"\"\n",
    "    A simple component that checks and prints GPU availability using PyTorch.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import logging\n",
    "    import os # Import os inside the function scope as well\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(asctime)s:%(message)s')\n",
    "\n",
    "    # Get the actual image name from the environment variable set by KFP\n",
    "    actual_base_image = os.getenv('KFP_COMPONENT_IMAGE')\n",
    "    if actual_base_image:\n",
    "        logging.info(f\"Running inside Base Image: {actual_base_image}\")\n",
    "    else:\n",
    "        # This case is less likely in KFP v2+, but good practice\n",
    "        logging.warning(\"Could not determine base image from KFP_COMPONENT_IMAGE env variable.\")\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"PyTorch version: {torch.__version__}\")\n",
    "        # Check if CUDA is available at all (drivers found, PyTorch CUDA compiled)\n",
    "        if not torch.cuda.is_available():\n",
    "            logging.warning(\"PyTorch CUDA is NOT available. GPU acceleration is NOT possible.\")\n",
    "            # You could add more diagnostics here if needed\n",
    "        else:\n",
    "            # Get the number of available GPUs\n",
    "            gpu_count = torch.cuda.device_count()\n",
    "            if gpu_count == 0:\n",
    "                logging.warning(\"PyTorch CUDA is available, but no GPUs were found by PyTorch.\")\n",
    "            else:\n",
    "                logging.info(f\"PyTorch found {gpu_count} GPU(s).\")\n",
    "                # List details for each GPU\n",
    "                for i in range(gpu_count):\n",
    "                    gpu_name = torch.cuda.get_device_name(i)\n",
    "                    gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3) # GiB\n",
    "                    logging.info(f\"  GPU {i}: {gpu_name} - Memory: {gpu_memory:.2f} GiB\")\n",
    "\n",
    "                # Optional: Add a small test computation on the first GPU\n",
    "                try:\n",
    "                    device = torch.device(f'cuda:{0}') # Use first GPU\n",
    "                    tensor_a = torch.randn(3, 3).to(device)\n",
    "                    tensor_b = torch.randn(3, 3).to(device)\n",
    "                    result = torch.matmul(tensor_a, tensor_b)\n",
    "                    logging.info(f\"Simple PyTorch computation on GPU {0} successful: First element={result[0,0].item()}\")\n",
    "                except Exception as compute_e:\n",
    "                     logging.error(f\"PyTorch detected GPU but failed test computation: {compute_e}\", exc_info=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while checking for GPUs with PyTorch: {e}\", exc_info=True) # Log traceback\n",
    "        # Re-raise the exception to make the KFP step fail\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da513971-4d3f-4946-b98d-d231d8d9fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Example Pipeline Definition (modify as needed) ---\n",
    "@dsl.pipeline(\n",
    "    name='gpu-test-pipeline-pt',\n",
    "    description='A minimal pipeline to test GPU access using PyTorch.'\n",
    ")\n",
    "def pytorch_gpu_test_pipeline():\n",
    "    \"\"\"Defines the pipeline structure.\"\"\"\n",
    "    check_gpu_task = (\n",
    "        check_gpu_access_pytorch() # Call the PyTorch version\n",
    "        .set_display_name(\"Check PyTorch GPU Availability\")\n",
    "        .set_gpu_limit(1)\n",
    "        # Ensure this label exists on your GPU nodes\n",
    "        .add_node_selector_constraint('nvidia.com/gpu')\n",
    "        .set_memory_limit(\"4G\")\n",
    "        .set_memory_request(\"2G\")\n",
    "        .set_cpu_limit(\"1\")\n",
    "        .set_cpu_request(\"0.5\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5cd8544-513f-436e-9c04-0a1f8fe1d611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/kfp/client/client.py:159: FutureWarning: This client only works with Kubeflow Pipeline v2.0.0-beta.2 and later versions.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/ceeeb68d-a201-4e62-b693-c2997177635e\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/4148aa0d-45c8-4835-b90e-890bc7821458\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=4148aa0d-45c8-4835-b90e-890bc7821458)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = kfp.Client()\n",
    "client.create_run_from_pipeline_func(\n",
    "    pytorch_gpu_test_pipeline,\n",
    "    experiment_name=\"mnist_pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527333e-b796-4af6-93a9-e2dc44882dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
